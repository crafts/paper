\chapter{Introduction}
Modern web services can see well over a billion requests per day. This sort of scale, as well as the advent of ``big data,'' has created a need for computational resources like never before. Data and services at such scale require advanced software and large amounts of computational resources to process requests in reasonable time.

In the early 2000s, handling a growing user base required the requisitioning of new hardware based on projected needs. This hardware was either purchased and maintained directly by the service, or leased on a month-to-month basis from a hosting provider. The process of predicting this demand is known as ``capacity planning \cite{allspaw2008art}.'' When planning, resource requirements are based on the maximum utilization a system will see, plus 15 to 20\% to handle future growth and unexpected demand. This model, however, leads to a problem: much of the service's resources will be wasted during non-peak times. For example, let's say you own an arts and crafts store called Glitter. Your customers live primarily in the U.S., so you see much more traffic during the day than you do at night. Additionally, your website can see more than double the amount of users around Mardi Gras. In order to handle this capacity, you must carefully plan your resources. Services need enough hardware to handle the demand at its worst, even though most of that capacity will not be used at non-peak times. Estimates claim that many services were only using 10-15\% of their total capacity on average \cite{vogels2008beyond}. This wasted capacity translates directly to financial waste.

In 2001, VMWare launched ESX and GSX server virtualization software \cite{vmware}. Virtualization allows many operating systems to run in isolation and share the physical resources of a single machine \cite{virtualization}. This allowed businesses to consolidate many applications, which would normally be required to run in isolation, to operate on a single physical machine and make better utilization of resources. In 2006, Amazon changed the IT landscape with the launch of Elastic Compute Cloud (EC2) \cite{ec2}. ``Cloud computing'' could be traced back all the way back to time-sharing mainframes in the 1950s \cite{strachey1959time}, but EC2 truly changed the game. Using virtualization technology, Amazon allowed customers to start and stop ``instances,'' virtual machines of varying sizes, quickly and easily. As other hosting providers quickly followed suit, acquisition of new computing resources became as easy as the push of a button. Capacity planning became less about planning and more about optimization. How can we waste the least amount of resources possible at any given time?

Now, entire applications are hosted in the cloud on these virtualized servers. Many of the pains of traditional capacity planning have been erased, but optimal utilization has still not been achieved. Services still see predictable changes in load and will often deactivate instances which are being underutilized, but this is often done using scheduling mechanisms that must be manually defined and cannot adapt to changing user demand and application performance. In order to most effectively utilize cloud resources, services need to be able to predict future traffic and preemptively scale their resources accordingly. This would allow capacity to follow demand as closely as possible.

In this document, we present CRAFTS (Cloud Resource Anticipation For Timing Scaling), a system for automatically identifying application throughput and predictively scaling cloud computing resources based on historical data. By taking past monitoring data such as requests per second and request latency, CRAFTS calculates the optimal throughput of the application it is monitoring and uses this data to make a direct translation between incoming traffic and the number of servers required to handle the capacity and maintain availability.

We also present ARTS (Automated Request Trace Simulator), a request-based workload generation tool for constructing diverse and realistic request patterns for modern web applications. ARTS allows us to evaluate CRAFTS' algorithms on a wide range of scenarios.

This document presents the following contributions:

\begin{itemize}
    \item CRAFTS, a prediction framework for cloud resource needs.
    \item ARTS, a tool for creating complex simulated request-based workload traces.
    \item A real world request-based workload based on 2007 Wikipedia traffic.
    \item A methodology for automatically tuning parameters of prediction algorithms applied to temporal data.
    \item An evaluation of several prediction algorithms applied to request-based traffic patterns.
\end{itemize}

The remainder of this document is organized as follows. \Cref{ch:background,ch:related} describe necessary background and related work and \Cref{ch:overview} provides a high-level overview of both ARTS and CRAFTS and outlines their respective requirements. \Cref{ch:datapipe,ch:architecture} focus on the design and implementation of CRAFTS with \Cref{ch:policies} outlining the prediction algorithms it implements. \Cref{ch:arts} discusses the design of ARTS and \Cref{ch:workloads} discusses both the ARTS-generated and real-world workloads used to evaluate CRAFTS. The remaining chapters explain the results of our evaluation of CRAFTS and conclusions as well as future work.